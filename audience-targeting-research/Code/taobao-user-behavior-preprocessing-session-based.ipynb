{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6284333,"sourceType":"datasetVersion","datasetId":3613679}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Such preparation is essential for training **multi-task models** that learn to predict multiple user intentions simultaneously, while preserving the **temporal dependencies** between actions.\n\n## Tasks Performed\n\n- Load and clean the original dataset\n- Filter users with fewer than 10 actions\n- Enforce correct behavioral sequence by removing out-of-order records\n- Generate session IDs based on 30-minute inactivity windows\n- Split the data into training, validation, and test sets based on timestamp\n- Save each set as a compressed CSV file\n\n## Dataset Schema\n\nAfter preprocessing, the dataset contains the following columns:\n- `UserID`: unique identifier for the user\n- `ItemID`: unique identifier for the item\n- `CategoryID`: identifier for the item's category\n- `pv`, `fav`, `cart`, `buy`: binary columns indicating behavior type\n- `SessionID`: identifier for a browsing session\n\n## File Outputs\n\n- `train_taobao_process.csv.gz` (2017-11-25 to 2017-11-28)\n- `val_taobao_process.csv.gz` (2017-11-29 to 2017-11-30)\n- `test_taobao_process.csv.gz` (2017-12-01 to 2017-12-03)\n\n## Environment\n\nThis project uses [PySpark](https://spark.apache.org/docs/latest/api/python/) for distributed preprocessing. Tested in a Kaggle environment with Spark 3.x.\n\n## How to Run\n\n1. You can access the dataset directly in a Kaggle notebook using the following path:\n\n`/kaggle/input/userbehavior/UserBehavior.csv`\nDataset link: [marwa80/userbehavior](https://www.kaggle.com/datasets/marwa80/userbehavior)\n   Use this path to read the CSV file using pandas or PySpark.\n3. Run the full preprocessing script (`.ipynb` or `.py`) provided in this repository\n4. The processed files will be saved to `/kaggle/working/`\n\n## Use Cases\n\nThis processed dataset is ideal for:\n- Sequential recommendation modeling\n- Multi-task learning (e.g., joint prediction of clicks and purchases)\n- Session-based prediction models\n- Contrastive learning and temporal pattern mining\n\n## Citation\n\nIf you use this code or the preprocessed dataset, please cite:\n\nMarwa Hamdi El-Sherief, Mohamed Helmy Khafagy and Asmaa Hashem Sweidan, “Multitask Model with an Attention Mechanism for Sequentially Dependent Online User Behaviors to Enhance Audience Targeting” International Journal of Advanced Computer Science and Applications(IJACSA), 16(4), 2025. http://dx.doi.org/10.14569/IJACSA.2025.01604112\n\nDataset: Marwa Hamdi. (2025). Preprocessed Taobao User Behavior Dataset for Sequential Modeling. Kaggle. https://www.kaggle.com/datasets/marwa80/userbehavior\n\n\n## License\n\nThis project is for academic and non-commercial use only.\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:39:22.208581Z","iopub.execute_input":"2025-07-01T10:39:22.209067Z","iopub.status.idle":"2025-07-01T10:39:24.721336Z","shell.execute_reply.started":"2025-07-01T10:39:22.208942Z","shell.execute_reply":"2025-07-01T10:39:24.719814Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/userbehavior/UserBehavior.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_timestamp, when, lag, unix_timestamp, sum as spark_sum\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType\n\n# Step 1: Create Spark session\nspark = SparkSession.builder.appName(\"TaobaoPreprocessing\").getOrCreate()\n\n# Step 2: Load the dataset\ndf = spark.read.csv(\"/kaggle/input/userbehavior/UserBehavior.csv\", header=False)\ndf = df.withColumnRenamed(\"_c0\", \"UserID\") \\\n       .withColumnRenamed(\"_c1\", \"ItemID\") \\\n       .withColumnRenamed(\"_c2\", \"CategoryID\") \\\n       .withColumnRenamed(\"_c3\", \"Behavior\") \\\n       .withColumnRenamed(\"_c4\", \"Timestamp\")\n\n# Step 3: Convert timestamp to datetime format\ndf = df.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"long\"))\ndf = df.withColumn(\"Datetime\", to_timestamp(col(\"Timestamp\")))\n\n# Step 4: Filter data within the target date range\ndf = df.filter((col(\"Datetime\") >= \"2017-11-25\") & (col(\"Datetime\") <= \"2017-12-03\"))\n\n# Step 5: Create binary columns for each behavior type\ndf = df.withColumn(\"pv\", when(col(\"Behavior\") == \"pv\", 1).otherwise(0)) \\\n       .withColumn(\"fav\", when(col(\"Behavior\") == \"fav\", 1).otherwise(0)) \\\n       .withColumn(\"cart\", when(col(\"Behavior\") == \"cart\", 1).otherwise(0)) \\\n       .withColumn(\"buy\", when(col(\"Behavior\") == \"buy\", 1).otherwise(0)) \\\n       .drop(\"Behavior\", \"Timestamp\")\n\n# Step 6: Keep only users with at least 10 actions\nuser_counts = df.groupBy(\"UserID\").count()\nvalid_users = user_counts.filter(col(\"count\") >= 10).select(\"UserID\")\ndf = df.join(valid_users, on=\"UserID\", how=\"inner\")\n\n# Step 7: Enforce correct behavior sequence (pv → fav → cart → buy)\ndf = df.withColumn(\"Step\",\n                   when(col(\"pv\") == 1, 1)\n                   .when(col(\"fav\") == 1, 2)\n                   .when(col(\"cart\") == 1, 3)\n                   .when(col(\"buy\") == 1, 4)\n                   .otherwise(0))\n\nw = Window.partitionBy(\"UserID\").orderBy(\"Datetime\")\ndf = df.withColumn(\"PrevStep\", lag(\"Step\").over(w))\ndf = df.withColumn(\"Valid\", when((col(\"PrevStep\").isNull()) | (col(\"Step\") >= col(\"PrevStep\")), 1).otherwise(0))\ndf = df.filter(col(\"Valid\") == 1).drop(\"PrevStep\", \"Valid\", \"Step\")\n\n# Step 8: Generate session IDs based on time difference > 30 minutes\ndf = df.withColumn(\"TimeDiff\", unix_timestamp(col(\"Datetime\")) - unix_timestamp(lag(\"Datetime\").over(w)))\ndf = df.withColumn(\"NewSession\", when((col(\"TimeDiff\") > 1800) | col(\"TimeDiff\").isNull(), 1).otherwise(0))\ndf = df.withColumn(\"SessionID\", spark_sum(\"NewSession\").over(w))\ndf = df.drop(\"TimeDiff\", \"NewSession\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:41:15.427870Z","iopub.execute_input":"2025-07-01T10:41:15.428662Z","iopub.status.idle":"2025-07-01T10:41:16.566383Z","shell.execute_reply.started":"2025-07-01T10:41:15.428617Z","shell.execute_reply":"2025-07-01T10:41:16.565360Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.show(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:41:44.394308Z","iopub.execute_input":"2025-07-01T10:41:44.394703Z","iopub.status.idle":"2025-07-01T10:45:26.567850Z","shell.execute_reply.started":"2025-07-01T10:41:44.394672Z","shell.execute_reply":"2025-07-01T10:45:26.566284Z"}},"outputs":[{"name":"stderr","text":"[Stage 6:>                                                                              (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-------+-------+----------+-------------------+---+---+----+---+---------+\n| UserID| ItemID|CategoryID|           Datetime| pv|fav|cart|buy|SessionID|\n+-------+-------+----------+-------------------+---+---+----+---+---------+\n|1000015|3243563|   2394030|2017-11-26 00:46:00|  1|  0|   0|  0|        1|\n|1000015|4758554|   4339722|2017-11-26 08:16:21|  1|  0|   0|  0|        2|\n|1000015|1711102|   2394030|2017-11-26 08:17:48|  1|  0|   0|  0|        2|\n|1000015|5116964|   3002561|2017-11-28 02:26:50|  1|  0|   0|  0|        3|\n|1000015|4005236|   3002561|2017-11-28 02:29:47|  1|  0|   0|  0|        3|\n|1000015|2006192|   3002561|2017-11-28 02:30:28|  1|  0|   0|  0|        3|\n|1000015| 592457|   3002561|2017-11-28 02:30:47|  1|  0|   0|  0|        3|\n|1000015|5012596|   3607361|2017-11-29 01:15:52|  1|  0|   0|  0|        4|\n|1000015| 999099|   3002561|2017-11-29 01:16:44|  1|  0|   0|  0|        4|\n|1000015|1317359|   3002561|2017-11-29 01:42:01|  0|  0|   1|  0|        4|\n|1000015|1317359|   3002561|2017-11-29 01:44:20|  1|  0|   0|  0|        4|\n|1000015|3609981|   3002561|2017-11-29 01:59:24|  1|  0|   0|  0|        4|\n|1000015|1317359|   3002561|2017-11-29 01:59:32|  1|  0|   0|  0|        4|\n|1000015|4806499|   4690421|2017-11-29 02:19:36|  1|  0|   0|  0|        4|\n|1000015|2520400|   3002561|2017-11-29 02:19:56|  1|  0|   0|  0|        4|\n|1000015|4648514|   3002561|2017-11-29 02:20:37|  1|  0|   0|  0|        4|\n|1000015|2433985|   2982027|2017-11-29 02:21:58|  1|  0|   0|  0|        4|\n|1000015|3352225|   3002561|2017-11-29 02:22:55|  1|  0|   0|  0|        4|\n|1000015|5002052|   3002561|2017-11-29 02:24:49|  1|  0|   0|  0|        4|\n|1000015|1317359|   3002561|2017-11-29 02:26:47|  1|  0|   0|  0|        4|\n+-------+-------+----------+-------------------+---+---+----+---+---------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Step 9: Split into train, validation, and test sets\ntrain_start = \"2017-11-25\"\ntrain_end   = \"2017-11-28\"\nval_start   = \"2017-11-29\"\nval_end     = \"2017-11-30\"\ntest_start  = \"2017-12-01\"\ntest_end    = \"2017-12-03\"\n\ntrain_df = df.filter((col(\"Datetime\") >= train_start) & (col(\"Datetime\") <= train_end)).drop(\"Datetime\")\nval_df   = df.filter((col(\"Datetime\") >= val_start)   & (col(\"Datetime\") <= val_end)).drop(\"Datetime\")\ntest_df  = df.filter((col(\"Datetime\") >= test_start)  & (col(\"Datetime\") <= test_end)).drop(\"Datetime\")\n\n# Step 10: Save output files as compressed CSV\ntrain_df.write.csv(\"/kaggle/working/train_taobao_process.csv.gz\", header=True, compression=\"gzip\")\nval_df.write.csv(\"/kaggle/working/val_taobao_process.csv.gz\", header=True, compression=\"gzip\")\ntest_df.write.csv(\"/kaggle/working/test_taobao_process.csv.gz\", header=True, compression=\"gzip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:06:58.833959Z","iopub.execute_input":"2025-07-01T11:06:58.836635Z","iopub.status.idle":"2025-07-01T11:25:34.806671Z","shell.execute_reply.started":"2025-07-01T11:06:58.836561Z","shell.execute_reply":"2025-07-01T11:25:34.804223Z"}},"outputs":[{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":5}]}