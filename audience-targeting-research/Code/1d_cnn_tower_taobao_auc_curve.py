{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7119392,"sourceType":"datasetVersion","datasetId":4106132}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:54.897792Z\",\"iopub.execute_input\":\"2024-02-13T09:09:54.898144Z\",\"iopub.status.idle\":\"2024-02-13T09:09:54.913442Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:54.898113Z\",\"shell.execute_reply\":\"2024-02-13T09:09:54.911893Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:54.915887Z\",\"iopub.execute_input\":\"2024-02-13T09:09:54.916459Z\",\"iopub.status.idle\":\"2024-02-13T09:09:54.920506Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:54.916423Z\",\"shell.execute_reply\":\"2024-02-13T09:09:54.919789Z\"}}\nmodel_file = '/kaggle/working/new_LT.model'\nval_path='/kaggle/input/taobao-features-engineer/val100_taobao_sample_user (2).csv/val100_taobao_sample_user.csv'\ntest_path='/kaggle/input/taobao-features-engineer/test100_taobao_sample_user (2).csv/test100_taobao_sample_user.csv'\ntrain_path='/kaggle/input/taobao-features-engineer/train100_taobao_sample_user (2).csv/train100_taobao_sample_user.csv'\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:54.921370Z\",\"iopub.execute_input\":\"2024-02-13T09:09:54.922996Z\",\"iopub.status.idle\":\"2024-02-13T09:09:54.934219Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:54.922937Z\",\"shell.execute_reply\":\"2024-02-13T09:09:54.932538Z\"}}\nuse_columns = [\n    '102', #UserID \n    '103',#user_size\n    '104',#SessionID\n     '105', #ItemID\n    '106',#CategoryID   \n    \n]\n\nvocabulary_size = {\n    '102':1028011 ,#UserID\n    '103':1000,#user_size\n    '104':16424022,  #SessionID\n    '105':5173070, #ItemID\n    '106':5172429,    #CategoryID   \n}\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:54.936088Z\",\"iopub.execute_input\":\"2024-02-13T09:09:54.937509Z\",\"iopub.status.idle\":\"2024-02-13T09:09:54.949340Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:54.937450Z\",\"shell.execute_reply\":\"2024-02-13T09:09:54.947686Z\"}}\n# data loader to load the data to the model\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\n\nclass XXDataset(Dataset):\n  '''load csv data with feature name ad first row'''\n  def __init__(self, datafile):\n    super(XXDataset, self).__init__()\n    self.feature_names = []\n    self.datafile = datafile\n    self.data = []\n    self._load_data()\n\n  def _load_data(self):\n    print(\"start load data from: {}\".format(self.datafile))\n    count = 0\n    with open(self.datafile) as f:\n      #self.feature_names = f.readline().strip().split(',')[2:]\n      self.feature_names = use_columns\n      for line in f:\n        if count == 0:### skip header \n          count += 1###\n          continue#####\n        count += 1\n        line = line.strip().split(',')\n        line = [int(v) for v in line]\n        self.data.append(line)\n        if count==10000000:########################## control number of rows\n            break\n    print(\"load data from {} finished\".format(self.datafile))\n  def __len__(self, ):\n    return len(self.data)\n  def __getitem__(self, idx):\n    line = self.data[idx]\n    pv = line[0]\n    buy = line[1]\n    features = dict(zip(self.feature_names, line[2:]))\n    return pv, buy, features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:54.950691Z\",\"iopub.execute_input\":\"2024-02-13T09:09:54.951139Z\",\"iopub.status.idle\":\"2024-02-13T09:09:54.968251Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:54.951109Z\",\"shell.execute_reply\":\"2024-02-13T09:09:54.966975Z\"}}\n#get data loader\ndef get_dataloader(filename, batch_size, shuffle):\n    data = XXDataset(filename)\n    loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n    return loader\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:09:59.237988Z\",\"iopub.execute_input\":\"2024-02-13T09:09:59.238448Z\",\"iopub.status.idle\":\"2024-02-13T09:09:59.445870Z\",\"shell.execute_reply.started\":\"2024-02-13T09:09:59.238405Z\",\"shell.execute_reply\":\"2024-02-13T09:09:59.444836Z\"}}\nbatch_size = 1\ntrain_dataloader = get_dataloader(train_path,\n                                  batch_size,\n                                  shuffle=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:00.098948Z\",\"iopub.execute_input\":\"2024-02-13T09:10:00.099285Z\",\"iopub.status.idle\":\"2024-02-13T09:10:00.152454Z\",\"shell.execute_reply.started\":\"2024-02-13T09:10:00.099260Z\",\"shell.execute_reply\":\"2024-02-13T09:10:00.151213Z\"}}\nval_dataloader = get_dataloader(val_path,\n                                  batch_size,\n                                 shuffle=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:00.651277Z\",\"iopub.execute_input\":\"2024-02-13T09:10:00.651934Z\",\"iopub.status.idle\":\"2024-02-13T09:10:00.680449Z\",\"shell.execute_reply.started\":\"2024-02-13T09:10:00.651887Z\",\"shell.execute_reply\":\"2024-02-13T09:10:00.679655Z\"}}\n\n###### first without modefication\nimport torch\nfrom torch import nn\n\nclass Tower2(nn.Module):\n  def __init__(self,\n               input_dim: int,\n               dims=[128, 64, 32],\n               drop_prob=[0.1, 0.3, 0.3]):\n    super(Tower2, self).__init__()\n    self.dims = dims\n    self.drop_prob = drop_prob\n    self.layer = nn.Sequential(nn.Linear(input_dim, dims[0]), nn.ReLU(),\n                               nn.Dropout(drop_prob[0]),\n                               nn.Linear(dims[0], dims[1]), nn.ReLU(),\n                               nn.Dropout(drop_prob[1]),\n                               nn.Linear(dims[1], dims[2]), nn.ReLU(),\n                               nn.Dropout(drop_prob[2]))\n\n  def forward(self, x):\n    \n    x = torch.flatten(x, start_dim=1)\n    \n    x = self.layer(x)\n    \n    return x \n    \n    \nclass Tower(nn.Module):\n    def __init__(self,\n               input_dim: int,\n               dims=[128, 64, 32],\n               drop_prob=[0.1, 0.3, 0.3],conv_dim = 1):\n        super(Tower, self).__init__()\n        self.dims = dims\n        self.drop_prob = drop_prob\n        self.kernal_size = 5\n        self.layer = nn.Sequential(nn.Linear(input_dim, dims[1]), nn.ReLU(),\n                               nn.Dropout(drop_prob[0]),\n                             \n                               nn.Linear(dims[1], dims[2]), nn.ReLU(),\n                               nn.Dropout(drop_prob[2])\n                             )\n                                  \n                            \n        ## conv1D\n        self.conv = nn.Sequential(\n                   \n             \n                     torch.nn.Conv1d(conv_dim,64,7, stride=1, padding=3), nn.ReLU(),\n                               nn.Dropout(drop_prob[1]),\n                     torch.nn.Conv1d(64,32,7, stride=1, padding=3), nn.ReLU(),\n                               nn.Dropout(drop_prob[1]),\n                     torch.nn.Conv1d(32, conv_dim,7, stride=1, padding=3) )\n\n    def forward(self, x):\n       \n        x = self.conv(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.layer(x)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, dim=32):\n        super(Attention, self).__init__()\n        self.dim = dim\n        self.q_layer = nn.Linear(dim, dim, bias=False)\n        self.k_layer = nn.Linear(dim, dim, bias=False)\n        self.v_layer = nn.Linear(dim, dim, bias=False)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, inputs):\n        Q = self.q_layer(inputs)\n        K = self.k_layer(inputs)\n        V = self.v_layer(inputs)\n        a = torch.sum(torch.mul(Q, K), -1) / torch.sqrt(torch.tensor(self.dim))\n        a = self.softmax(a)\n        outputs = torch.sum(torch.mul(torch.unsqueeze(a, -1), V), dim=1)\n        return outputs\n\nclass PM(nn.Module):\n    def __init__(self,\n               feature_vocabulary: dict[str, int],\n               embedding_size: int,\n               tower_dims=[128, 64, 32],\n               drop_prob=[0.1, 0.3, 0.3],batch_size=1):\n        super(PM, self).__init__()\n        self.feature_vocabulary = feature_vocabulary\n        self.feature_names = sorted(list(feature_vocabulary.keys()))\n        self.embedding_size = embedding_size\n        self.embedding_dict = nn.ModuleDict()\n        self.__init_weight()\n        self.tower_dims = tower_dims\n        self.drop_prob = drop_prob\n        self.tower_input_size = len(feature_vocabulary) * embedding_size\n        # self.click_tower = Tower(self.tower_input_size, tower_dims, drop_prob)\n        self.click_tower =Tower(self.tower_input_size, self.tower_dims, self.drop_prob,batch_size)\n        self.conversion_tower = Tower2(self.tower_input_size, tower_dims, drop_prob)\n        self.attention_layer = Attention(tower_dims[-1])\n        self.info_layer = nn.Sequential(nn.Linear(tower_dims[-1], 32), nn.ReLU(),\n                                    nn.Dropout(drop_prob[-1]))\n\n        self.click_layer = nn.Sequential(nn.Linear(tower_dims[-1], 1),\n                                     nn.Sigmoid())\n        self.conversion_layer = nn.Sequential(nn.Linear(tower_dims[-1], 1),\n                                          nn.Sigmoid())\n\n    def __init_weight(self, ):\n        for name, size in self.feature_vocabulary.items():\n            emb = nn.Embedding(size, self.embedding_size)\n            nn.init.normal_(emb.weight, mean=0.0, std=0.01)\n            self.embedding_dict[name] = emb\n    def forward(self, x):\n        feature_embedding = []\n        for name in self.feature_names:\n            embed = self.embedding_dict[name](x[name])\n            feature_embedding.append(embed)\n        feature_embedding = torch.cat(feature_embedding, 1)\n        tower_click = self.click_tower(feature_embedding)\n        tower_conversion = torch.unsqueeze(\n        self.conversion_tower(feature_embedding), 1)\n\n        info = torch.unsqueeze(self.info_layer(tower_click), 1)\n\n        ait = self.attention_layer(torch.cat([tower_conversion, info], 1))\n\n        click = torch.squeeze(self.click_layer(tower_click), dim=1)\n        conversion = torch.squeeze(self.conversion_layer(ait), dim=1)\n        \n        return click, conversion\n\n    def loss(self,\n           click_label,\n           click_pred,\n           conversion_label,\n           conversion_pred,\n           constraint_weight=0.6,\n           device=\"gpu:1\"):\n        click_label = click_label.to(device)\n        conversion_label = conversion_label.to(device)\n        click_loss = nn.functional.binary_cross_entropy(click_pred, click_label)\n        conversion_loss = nn.functional.binary_cross_entropy(\n        conversion_pred, conversion_label)\n        label_constraint = torch.maximum(conversion_pred - click_pred,\n                                     torch.zeros_like(click_label))\n        \n        constraint_loss = torch.sum(label_constraint)\n        \n        loss = click_loss + conversion_loss + constraint_weight * constraint_loss\n        return loss\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:01.138018Z\",\"iopub.execute_input\":\"2024-02-13T09:10:01.138689Z\",\"iopub.status.idle\":\"2024-02-13T09:10:01.146557Z\",\"shell.execute_reply.started\":\"2024-02-13T09:10:01.138646Z\",\"shell.execute_reply\":\"2024-02-13T09:10:01.144784Z\"}}\n#next(iter(train_dataloader))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:01.627244Z\",\"iopub.execute_input\":\"2024-02-13T09:10:01.627733Z\",\"iopub.status.idle\":\"2024-02-13T09:10:01.639852Z\",\"shell.execute_reply.started\":\"2024-02-13T09:10:01.627690Z\",\"shell.execute_reply\":\"2024-02-13T09:10:01.637912Z\"}}\n# train\nimport sys\nimport random\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nfrom sklearn.metrics import roc_curve\n\n# super parameter\nbatch_size = 1\nembedding_size = 5\nlearning_rate = 0.0001\ntotal_epoch= 5\nearlystop_epoch = 1\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:02.232689Z\",\"iopub.execute_input\":\"2024-02-13T09:10:02.233060Z\",\"iopub.status.idle\":\"2024-02-13T09:10:02.261659Z\",\"shell.execute_reply.started\":\"2024-02-13T09:10:02.233033Z\",\"shell.execute_reply\":\"2024-02-13T09:10:02.258660Z\"}}\ntrain_loss_list=[]\n\ndef train():\n  # train_dataloader = get_dataloader(train_path,\n  #                                   batch_size,\n  #                                   shuffle=True)\n  # dev_dataloader = get_dataloader(val_path,\n  #                                 batch_size,\n  #                                 shuffle=True)\n  model = PM(vocabulary_size, embedding_size)\n  device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n  optimizer = torch.optim.Adam(model.parameters(),\n                               lr=learning_rate,\n                               weight_decay=1e-6)\n  model.to(device)\n  best_acc = 0.0\n  earystop_count = 0\n  best_epoch = 0\n  for epoch in range(total_epoch):\n    total_loss = 0.\n    nb_sample = 0\n    # train\n    model.train()\n    for step, batch in enumerate(train_dataloader): # step => counter\n      #print(batch) # [click , convertion , {features}]\n      # [tensor([0]), tensor([0]), {'101': tensor([0]), '121': tensor([0]), '122': tensor([0]), '124': tensor([0]), '125': tensor([0]), '126': tensor([0]), '127': tensor([0]), '128': tensor([0]), '129': tensor([0]), '205': tensor([0]), '206': tensor([0]), '207': tensor([0]), '216': tensor([0]), '508': tensor([0]), '509': tensor([0]), '702': tensor([0]), '853': tensor([0]), '301': tensor([0])}]\n      click, conversion, features = batch\n      for key in features.keys():\n        features[key] = features[key].to(device)\n      click_pred, conversion_pred = model(features)\n      loss = model.loss(click.float(),\n                        click_pred,\n                        conversion.float(),\n                        conversion_pred,\n                        device=device)\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      total_loss += loss.cpu().detach().numpy()\n      nb_sample += click.shape[0]\n      if step % 100 == 0:\n        print('[%d] Train loss on step %d: %.6f' %\n              (nb_sample, (step + 1), total_loss / (step + 1)))\n        train_loss_list.append(total_loss / (step + 1))\n\n # validation\n    print(\"start validation...\")\n    click_pred = []\n    click_label = []\n    conversion_pred = []\n    conversion_label = []\n    model.eval()\n    for step, batch in enumerate(val_dataloader):\n      click, conversion, features = batch\n      for key in features.keys():\n        features[key] = features[key].to(device)\n\n      with torch.no_grad():\n        click_prob, conversion_prob = model(features)\n\n      click_pred.append(click_prob.cpu())\n      conversion_pred.append(conversion_prob.cpu())\n\n      click_label.append(click)\n      conversion_label.append(conversion)\n\n    click_auc = cal_auc(click_label, click_pred)\n    conversion_auc = cal_auc(conversion_label, conversion_pred)\n    print(\"Epoch: {} click_auc: {} conversion_auc: {}\".format(\n        epoch + 1, click_auc, conversion_auc))\n\n    acc = click_auc + conversion_auc\n    \n    fpr_click, tpr_click, _ = auc_curve(click_label, click_pred)\n    fpr_conversion, tpr_conversion, _ = auc_curve(conversion_label, conversion_pred)\n\n    plt.figure(1)\n    plt.plot([0, 1], [0, 1], 'k-')\n    plt.plot(fpr_click, tpr_click, label='Click (AUC = {:.3f})'.format(click_auc))\n    plt.plot(fpr_conversion, tpr_conversion, label='Conversion (AUC = {:.3f})'.format(conversion_auc))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='best')\n    plt.show()\n\n\n  torch.save(model.state_dict(), model_file)\n\ndef test():\n  print(\"Start Test ...\")\n  test_loader = get_dataloader(test_path,\n                               batch_size=batch_size,\n                               shuffle=False)\n  model = PM(vocabulary_size, 5)\n  model.load_state_dict(torch.load(model_file))\n  model.eval()\n  click_list = []\n  conversion_list = []\n  click_pred_list = []\n  conversion_pred_list = []\n  for i, batch in enumerate(test_loader):\n    if i % 100:\n      sys.stdout.write(\"test step:{}\\r\".format(i))\n      sys.stdout.flush()\n    click, conversion, features = batch\n    with torch.no_grad():\n      click_pred, conversion_pred = model(features)\n    click_list.append(click)\n    conversion_list.append(conversion)\n    click_pred_list.append(click_pred)\n    conversion_pred_list.append(conversion_pred)\n  click_auc = cal_auc(click_list, click_pred_list)\n  conversion_auc = cal_auc(conversion_list, conversion_pred_list)\n  print(\"Test Resutt: click AUC: {} conversion AUC:{}\".format(\n      click_auc, conversion_auc))\n\n  fpr_click, tpr_click, _ = auc_curve(click_list, click_pred_list)\n  fpr_conversion, tpr_conversion, _ = auc_curve(conversion_list, conversion_pred_list)\n\n  plt.figure(1)\n  plt.plot([0, 1], [0, 1], 'k-')\n  plt.plot(fpr_click, tpr_click, label='Click (AUC = {:.3f})'.format(click_auc))\n  plt.plot(fpr_conversion, tpr_conversion, label='Conversion (AUC = {:.3f})'.format(conversion_auc))\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.legend(loc='best')\n  plt.show()\n\n\n\n\ndef cal_auc(label: list, pred: list):\n  label = torch.cat(label)\n  pred = torch.cat(pred)\n  label = label.detach().numpy()\n  pred = pred.detach().numpy()\n  auc = roc_auc_score(label, pred, labels=np.array([0.0, 1.0]))\n  return auc\ndef auc_curve(label: list, pred: list):\n  label = torch.cat(label)\n  pred = torch.cat(pred)\n  label = label.detach().numpy()\n  pred = pred.detach().numpy()\n  auc = roc_auc_score(label, pred, labels=np.array([0.0, 1.0]))\n  fpr, tpr, _ = roc_curve(label, pred)\n  return fpr, tpr, _\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-13T09:10:03.395484Z\",\"iopub.execute_input\":\"2024-02-13T09:10:03.395905Z\"}}\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\ntrain()\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n# %% [code]\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-02-12T17:08:40.584399Z\",\"iopub.execute_input\":\"2024-02-12T17:08:40.584903Z\"}}\nstart_time = timer()\ntest()\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n# %% [code]\n\n# Replace 'your_model.pth' with the actual file path\nmodel_file = '/kaggle/working/new_PM.model'\n\n\n\n# Load the entire state of the model\ncheckpoint = torch.load(model_file)\n\n# Print the keys in the checkpoint dictionary\nprint(checkpoint.keys())\n\n\n# %% [code]\n","metadata":{"_uuid":"3c59c6b3-99c1-44fc-8b93-a7bee2aab7fe","_cell_guid":"a6d1ab6c-8ead-49a9-acfc-8936cc6d4818","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}